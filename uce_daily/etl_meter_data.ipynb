{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import ftplib\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import logging\n",
    "import pytz\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import NullPool\n",
    "\n",
    "from settings.db import WAREHOSUE_URL\n",
    "from settings.ftp import METER_DATA_FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ftp(ftp_settings):\n",
    "    ftp = ftplib.FTP()\n",
    "    ftp.connect(ftp_settings['host'], ftp_settings['port'])\n",
    "    # ftp.set_pasv(True)\n",
    "    ftp.login(ftp_settings['user'], ftp_settings['passwd'])\n",
    "    ftp.cwd(ftp_settings['working_dir'])\n",
    "    return ftp\n",
    "\n",
    "\n",
    "def get_forecast_files(ftp):\n",
    "    files = dict()\n",
    "    for file_name in ftp.nlst():\n",
    "        if file_name[-4:] == '.txt':\n",
    "            date = dt.datetime.strptime(file_name[5:11], '%d%m%y')\n",
    "            creation_time = ftp.sendcmd('MDTM ' + file_name)[4:]\n",
    "            creation_time = dt.datetime.strptime(creation_time, '%Y%m%d%H%M%S')\n",
    "            try:\n",
    "                files[date].update({creation_time: file_name})\n",
    "            except KeyError:\n",
    "                files[date] = {creation_time: file_name}\n",
    "    for date in sorted(files):\n",
    "        for time in sorted(files[date]):\n",
    "            yield date, time, files[date][time]\n",
    "\n",
    "\n",
    "def read_meter_data(ftp, file_name):\n",
    "    content = io.BytesIO()\n",
    "    ftp.retrbinary(f'RETR {file_name}', content.write)\n",
    "    content.seek(0)\n",
    "    return content\n",
    "\n",
    "\n",
    "def move_seen_file(ftp, old_filename, new_filename, results_dict):\n",
    "    ftp.rename(old_filename, new_filename)\n",
    "    results_dict.update({old_filename: True})\n",
    "    return True\n",
    "\n",
    "def get_dim_metering_poins(date, connection):\n",
    "    query = f\"\"\"\n",
    "    select * from dim_metering_point\n",
    "    where valid_from <= '{date.date()}'\n",
    "    and valid_to >= '{date.date()}'\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        'id',\n",
    "        'parameter_number',\n",
    "        'serial_number',\n",
    "        'name',\n",
    "        'site_id',\n",
    "        'parameter',\n",
    "        'z_code',\n",
    "        'valid_from',\n",
    "        'valid_to',\n",
    "        'is_active',\n",
    "        'is_border',\n",
    "        'is_inverter',\n",
    "        'weight_factor',\n",
    "    ]\n",
    "    response = pd.read_sql_query(query, connection)\n",
    "    metering_points = pd.DataFrame(response, columns=columns)\n",
    "    metering_points['metering_point_parameters'] = metering_points['serial_number'].astype('str') + metering_points['parameter_number'].astype('str')\n",
    "    metering_points.index = metering_points['metering_point_parameters']\n",
    "    return metering_points\n",
    "\n",
    "\n",
    "def add_dim_metering_poins(metering_point_ids, date, connection):\n",
    "    columns = [\n",
    "        'id',\n",
    "        'parameter_number',\n",
    "        'serial_number',\n",
    "        'name',\n",
    "        'site_id',\n",
    "        'parameter',\n",
    "        'z_code',\n",
    "        'valid_from',\n",
    "        'valid_to',\n",
    "        'is_active',\n",
    "        'is_border',\n",
    "        'is_inverter',\n",
    "        'weight_factor',\n",
    "    ]\n",
    "    parameters = {'1': 'A+', '2': 'A-', '3': 'R+', '4': 'R-'}\n",
    "    insert_statements = list()\n",
    "    for metering_point_id in metering_point_ids:\n",
    "        parameter_number = metering_point_id[-1]\n",
    "        serial_number = metering_point_id[:-1]\n",
    "        valid_from = date.strftime('%Y-%m-%d')\n",
    "        parameter = parameters[parameter_number]\n",
    "        insert_statement = \"('{}', '{}', 'noname', 100000, '{}', '62Z0000000000000', '{}', '9999-12-31', false, false, false, 1.0)\"\n",
    "        insert_statements.append(insert_statement.format(parameter_number, serial_number, parameter, valid_from))\n",
    "    query = f\"\"\"\n",
    "    insert into dim_metering_point({', '.join(columns[1:])}) VALUES {', '.join(insert_statements)};\n",
    "    \"\"\"\n",
    "    connection.execute(query)\n",
    "    return get_dim_metering_poins(date, connection)\n",
    "\n",
    "\n",
    "def prepare_meter_data(raw_meter_data, metering_points):\n",
    "    fact_meter_data = raw_meter_data.join(metering_points, on=0, how='left')\n",
    "    fact_meter_data['metering_point_id'] = fact_meter_data['id']\n",
    "    fact_meter_data['parameter'] = fact_meter_data['parameter_number']\n",
    "    fact_meter_data['date_id'] = pd.Series(index=fact_meter_data.index, data=file[0].strftime('%Y%m%d')).astype(int)\n",
    "    fact_meter_data['availability_utc'] = pd.Series(index=fact_meter_data.index, data=file[1].replace(tzinfo=pytz.timezone('europe/kiev')).astimezone(pytz.UTC).replace(tzinfo=None).strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "    fact_meter_data['daily_total'] = fact_meter_data[1]\n",
    "    fact_meter_data['sub_hourly_values'] = fact_meter_data.iloc[:, 2:52].agg(list, axis=1)\n",
    "    fact_meter_data = fact_meter_data.iloc[:, 52:]\n",
    "    fact_meter_data.drop(columns=['id', 'serial_number', 'parameter_number', 'name', 'z_code', \n",
    "                                'valid_from', 'valid_to', 'is_active', 'is_border', 'is_inverter', \n",
    "                                'weight_factor', 'metering_point_parameters'], inplace=True)\n",
    "    fact_meter_data_columns = ['date_id', 'site_id', 'metering_point_id', 'parameter', 'availability_utc', 'daily_total', 'sub_hourly_values']\n",
    "    fact_meter_data = fact_meter_data[fact_meter_data_columns]\n",
    "    return fact_meter_data\n",
    "\n",
    "\n",
    "def put_meter_data_to_warehouse(data, connection):\n",
    "    df_full = data.copy()\n",
    "    tuples_full = [str(tuple(x)).replace('[', \"'{\").replace(']', \"}'\") for x in df_full.to_numpy()]\n",
    "    # print(tuples_full[:2])\n",
    "    columns = list(df_full.columns)\n",
    "    columns_unique = columns[0:4]\n",
    "    # print(columns_unique)\n",
    "    # SQL query to execute\n",
    "    query_1 = 'INSERT INTO fact_meter_data({}) VALUES \\n{}\\n'.format(', '.join(columns), ', \\n'.join(tuples_full).replace('[', '{').replace(']', '}'))\n",
    "    query_2 = '''\n",
    "    ON CONFLICT (date_id, site_id, metering_point_id, parameter) \n",
    "    DO UPDATE SET\n",
    "    availability_utc = excluded.availability_utc,\n",
    "    daily_total = excluded.daily_total,\n",
    "    sub_hourly_values = excluded.sub_hourly_values;\n",
    "    '''\n",
    "    # print(query_1, query_2)\n",
    "    connection.execute(query_1 + '\\n' + query_2)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(WAREHOSUE_URL, poolclass=NullPool)\n",
    "ftp = make_ftp(METER_DATA_FTP)\n",
    "ftp_threads = list()\n",
    "ftp_move_results = dict()\n",
    "# print(ftp.pwd())\n",
    "files = get_forecast_files(ftp)\n",
    "for file in files:\n",
    "# file = next(files)\n",
    "    print(file)\n",
    "    raw_meter_data = read_meter_data(ftp, file[-1])\n",
    "    raw_meter_data = pd.read_csv(raw_meter_data, delimiter=':', skiprows=1, header=None, skipfooter=1, engine='python')\n",
    "    raw_meter_data[0] = raw_meter_data[0].apply(lambda x: x[1:-1])\n",
    "    raw_meter_data.drop(columns=[52], inplace=True)\n",
    "\n",
    "    duplicates = raw_meter_data[0].duplicated(keep=False)\n",
    "\n",
    "    duplicates = raw_meter_data.loc[duplicates]\n",
    "    # print(duplicates)\n",
    "    indexes_to_keep = list()\n",
    "    indexes_to_remove = list()\n",
    "\n",
    "    for id in duplicates[0].unique():\n",
    "        # print(id)\n",
    "        questionable_records = duplicates.loc[duplicates[0] == id]\n",
    "        # print(questionable_records)\n",
    "        if questionable_records[1].sum() == 0:\n",
    "            index_to_keep = questionable_records.index[0]\n",
    "            index_to_drop = questionable_records.index[1:].to_list()\n",
    "        else:\n",
    "            index_to_keep = questionable_records.loc[questionable_records[1] == questionable_records[1].max()].index.to_list()[0]\n",
    "            index_to_drop = list(set(questionable_records.index).difference(set([index_to_keep])))\n",
    "        indexes_to_keep.append(index_to_keep)\n",
    "        indexes_to_remove.extend(index_to_drop)\n",
    "        \n",
    "    # print('Keep: {}'.format(indexes_to_keep))\n",
    "    # print('Drop: {}'.format(indexes_to_remove))\n",
    "    raw_meter_data.drop(indexes_to_remove, inplace=True)\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        metering_points = get_dim_metering_poins(file[0], connection)\n",
    "        dim_metering_points_parameters = set(metering_points['metering_point_parameters'].to_list())\n",
    "        raw_meter_data_points_parameters = set(raw_meter_data[0].to_list())\n",
    "        absent_points = raw_meter_data_points_parameters - dim_metering_points_parameters\n",
    "        if absent_points:\n",
    "            # add newly comming metering points\n",
    "            print(f'Adding new metering points: {len(absent_points)}')\n",
    "            metering_points = add_dim_metering_poins(absent_points, file[0], connection)\n",
    "            dim_metering_points_parameters = set(metering_points['metering_point_parameters'].to_list())\n",
    "            raw_meter_data_points_parameters = set(raw_meter_data[0].to_list())\n",
    "            absent_points = raw_meter_data_points_parameters - dim_metering_points_parameters\n",
    "            print(len(absent_points))\n",
    "        fact_meter_data = prepare_meter_data(raw_meter_data, metering_points)\n",
    "        result = put_meter_data_to_warehouse(fact_meter_data, connection)\n",
    "        old_filename = file[-1]\n",
    "        old_filename = METER_DATA_FTP['working_dir'] + old_filename\n",
    "        new_filename = file[-1][:11] + '000000' + file[-1][-4:]\n",
    "        new_filename = METER_DATA_FTP['working_dir'] + 'archived/' + new_filename\n",
    "        ftp_thread = threading.Thread(target=move_seen_file, args=[ftp, old_filename, new_filename, ftp_move_results])\n",
    "        ftp_thread.start()\n",
    "        ftp_threads.append(ftp_thread)\n",
    "        print(result)\n",
    "        print('-----------------------------------')\n",
    "        time.sleep(5)\n",
    "\n",
    "for ftp_thread in ftp_threads:\n",
    "    ftp_thread.join()\n",
    "\n",
    "print(ftp_move_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6dd8836d127917c5dabc39bdf7e43817871ec0bc4a542a3d13d62efcce73dfc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('uce_daily')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
