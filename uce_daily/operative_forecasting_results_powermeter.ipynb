{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import time\n",
    "import calendar\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "from uce_resources import get_mms_data, get_applied_forecast, get_current_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings.sites import ceg as sites_list\n",
    "\n",
    "today = dt.datetime.today()\n",
    "\n",
    "target_year = today.year\n",
    "target_month = today.month\n",
    "forecasts_types = ['real']\n",
    "\n",
    "target_folder = 'data/results/{}-{:0>2}/'.format(target_year, target_month)\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "\n",
    "# sites_list = ['Solone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token updated!\n",
      "Site ids updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n",
      "Devices updated!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "imported_module = importlib.import_module(\"get_current_production_powermeter\")\n",
    "importlib.reload(imported_module)\n",
    "from get_current_production_powermeter import OperativeProduction\n",
    "from settings.apis import BORD_API_SETTINGS\n",
    "\n",
    "InverterDataGetter = OperativeProduction(**BORD_API_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o.babenko\\.conda\\envs\\uce_daily\\lib\\site-packages\\ipykernel_launcher.py:8: SAWarning: Did not recognize type 'point' of column 'location'\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.pool import NullPool\n",
    "from sqlalchemy.sql import select, and_\n",
    "from settings.db import DO_URL\n",
    "\n",
    "engine_source = create_engine(DO_URL, poolclass=NullPool)\n",
    "metadata_source = MetaData()\n",
    "metadata_source.reflect(bind=engine_source)\n",
    "\n",
    "\n",
    "from settings.db import WAREHOSUE_URL\n",
    "\n",
    "engine_warehouse = create_engine(WAREHOSUE_URL, poolclass=NullPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully: 5, 922\n",
      "Bar: ок! Processing took 4.55 seconds\n",
      "Data read successfully: 6, 902\n",
      "Verkhivka: ок! Processing took 3.69 seconds\n",
      "Data read successfully: 7, 921\n",
      "Balky: ок! Processing took 3.26 seconds\n",
      "Data read successfully: 8, 712\n",
      "Data read successfully: 8, 717\n",
      "Sharhorod_1: ок! Processing took 3.82 seconds\n",
      "Data read successfully: 10, 711\n",
      "Chechelnyk_1: ок! Processing took 3.41 seconds\n",
      "Data read successfully: 11, 747\n",
      "Chechelnyk_2: ок! Processing took 3.3 seconds\n",
      "Data read successfully: 12, 953\n",
      "Stanislavchyk: ок! Processing took 3.34 seconds\n",
      "Data read successfully: 13, 939\n",
      "Data read successfully: 13, 940\n",
      "Kopaihorod: ок! Processing took 4.27 seconds\n",
      "Data read successfully: 14, 573\n",
      "Data read successfully: 14, 575\n",
      "Cherniatka: ок! Processing took 3.87 seconds\n",
      "Data read successfully: 16, 794\n",
      "Hlybochok_1: ок! Processing took 3.52 seconds\n",
      "Data read successfully: 17, 796\n",
      "Hlybochok_2.1: ок! Processing took 3.66 seconds\n",
      "Data read successfully: 18, 790\n",
      "Data read successfully: 18, 791\n",
      "Hlybochok_2.2: ок! Processing took 3.62 seconds\n",
      "Data read successfully: 33, 34\n",
      "Pohrebyshche: ок! Processing took 3.6 seconds\n",
      "Data read successfully: 34, 29\n",
      "Bilashky: ок! Processing took 3.51 seconds\n",
      "Data read successfully: 51, 671\n",
      "Data read successfully: 51, 677\n",
      "Porohy: ок! Processing took 3.62 seconds\n",
      "Data read successfully: 53, 604\n",
      "Data read successfully: 53, 610\n",
      "Hnatkiv: ок! Processing took 3.63 seconds\n",
      "Data read successfully: 1, 346\n",
      "Myroliubivka: ок! Processing took 3.41 seconds\n",
      "Data read successfully: 3, 367\n",
      "Kyselivka: ок! Processing took 3.18 seconds\n",
      "Data read successfully: 4, 389\n",
      "Poniativka: ок! Processing took 4.14 seconds\n",
      "Data read successfully: 9, 511\n",
      "Kostohryzove: ок! Processing took 3.25 seconds\n",
      "Data read successfully: 15, 1054\n",
      "Bilozerka: ок! Processing took 3.23 seconds\n",
      "Data read successfully: 19, 846\n",
      "Mykolaivka: ок! Processing took 3.06 seconds\n",
      "Exception raised: 422 Client Error: Unprocessable Entity for url: https://k8s.ceg.energydata.tech/api/v1/mainapi/get/data/20/848\n",
      "Komyshany_1: ок! Processing took 3.42 seconds\n",
      "Exception raised: 422 Client Error: Unprocessable Entity for url: https://k8s.ceg.energydata.tech/api/v1/mainapi/get/data/21/849\n",
      "Komyshany_2: ок! Processing took 2.98 seconds\n",
      "Velihen: ок! Processing took 3.01 seconds\n",
      "Veliton: ок! Processing took 3.14 seconds\n",
      "Data read successfully: 25, 847\n",
      "Mala_Lepetykha: ок! Processing took 3.27 seconds\n",
      "Data read successfully: 26, 1088\n",
      "Data read successfully: 26, 1089\n",
      "Rubanivka: ок! Processing took 4.46 seconds\n",
      "Data read successfully: 27, 1034\n",
      "Data read successfully: 27, 1035\n",
      "Oleshky_2: ок! Processing took 3.81 seconds\n",
      "Data read successfully: 28, 1004\n",
      "Data read successfully: 28, 1005\n",
      "Oleshky_1: ок! Processing took 3.57 seconds\n",
      "Data read successfully: 31, 112\n",
      "Liubymivka: ок! Processing took 3.06 seconds\n",
      "Data read successfully: 49, 235\n",
      "Kachkarivka: ок! Processing took 3.23 seconds\n",
      "Data read successfully: 44, 139\n",
      "Data read successfully: 44, 150\n",
      "Data read successfully: 44, 183\n",
      "Vasylivka: ок! Processing took 4.27 seconds\n",
      "Data read successfully: 29, 1\n",
      "Data read successfully: 29, 6\n",
      "Afanasiivka: ок! Processing took 4.07 seconds\n",
      "Data read successfully: 2, 423\n",
      "Novokondakove: ок! Processing took 3.53 seconds\n",
      "Data read successfully: 30, 26\n",
      "Bazaltova: ок! Processing took 3.41 seconds\n",
      "Data read successfully: 35, 73\n",
      "Yelanets_1: ок! Processing took 3.17 seconds\n",
      "Data read successfully: 36, 74\n",
      "Yelanets_2: ок! Processing took 3.22 seconds\n",
      "Data read successfully: 38, 451\n",
      "Inhulets_1: ок! Processing took 3.46 seconds\n",
      "Data read successfully: 39, 484\n",
      "Inhulets_2: ок! Processing took 3.44 seconds\n",
      "Data read successfully: 40, 76\n",
      "Bereznehuvate: ок! Processing took 3.36 seconds\n",
      "Data read successfully: 41, 77\n",
      "Teplychna: ок! Processing took 3.37 seconds\n",
      "Data read successfully: 42, 441\n",
      "Solone: ок! Processing took 3.25 seconds\n",
      "Data read successfully: 43, 121\n",
      "Stepnohirsk: ок! Processing took 3.21 seconds\n",
      "Data read successfully: 32, 204\n",
      "Balivka: ок! Processing took 3.8 seconds\n",
      "Data read successfully: 22, 260\n",
      "Data read successfully: 22, 261\n",
      "Dibrovka: ок! Processing took 3.63 seconds\n",
      "Data read successfully: 52, 652\n",
      "Data read successfully: 52, 658\n",
      "Kulevcha: ок! Processing took 3.8 seconds\n"
     ]
    }
   ],
   "source": [
    "sites_data = list()\n",
    "\n",
    "with engine_source.connect() as connection:\n",
    "        \n",
    "    for site in sites_list:\n",
    "        start = time.time()\n",
    "        # print('-'*50)\n",
    "        # print(site)\n",
    "        site_data = dict()\n",
    "\n",
    "        sites_table = metadata_source.tables['sites']\n",
    "        list_to_select = [\n",
    "            sites_table.c.id, \n",
    "            sites_table.c.legal_entity, \n",
    "            sites_table.c.location, \n",
    "            sites_table.c.region, \n",
    "            sites_table.c.cluster,\n",
    "            sites_table.c.installed_capacity_dc,\n",
    "            sites_table.c.grid_capacity,\n",
    "            sites_table.c.w_code\n",
    "            ]\n",
    "        query = select(list_to_select).where(sites_table.c.displayable_name == site)\n",
    "        site_id_response = connection.execute(query).fetchall()[0]\n",
    "\n",
    "        site_id = site_id_response[0]\n",
    "        legal_entity_id = site_id_response[1]\n",
    "        location = site_id_response[2]\n",
    "        region = site_id_response[3]\n",
    "        cluster = site_id_response[4]\n",
    "        capacity_dc = site_id_response[5]\n",
    "        grid_capacity = site_id_response[6]\n",
    "        w_code = site_id_response[7]\n",
    "        \n",
    "        with engine_warehouse.connect() as connection_warehouse:\n",
    "            query = f\"\"\"\n",
    "            SELECT grid_capacity from dim_site\n",
    "            WHERE site_name = '{site}';\n",
    "            \"\"\"\n",
    "            response = connection_warehouse.execute(query).fetchall()[0]\n",
    "            \n",
    "        grid_capacity = response[0]\n",
    "\n",
    "        latitude, longitude = map(float, location.replace('(', '').replace(')', '').split(','))\n",
    "        mms_data, mms_version = get_mms_data(site_id, \n",
    "                                             target_year, target_month, \n",
    "                                             connection, metadata_source.tables['mms_data'], include_prev=True,)\n",
    "        mms_data.columns = ['yield']\n",
    "        # print(mms_data)\n",
    "        # print('MMS data | {} version | of | {} records |'.format(mms_version, len(mms_data)))\n",
    "        # print(mms_data.index.max())\n",
    "\n",
    "        inverters_data = InverterDataGetter.get_data(w_code, today)\n",
    "        # print(inverters_data)\n",
    "        if inverters_data is not None and len(inverters_data) > 0:\n",
    "            mms_data = pd.concat([mms_data, inverters_data], axis=0, join='inner')\n",
    "            #print(mms_data.loc[mms_data.index.intersection(inverters_data.index)])\n",
    "            #raise KeyboardInterrupt\n",
    "        first_date = dt.date(target_year, target_month, 1),\n",
    "        last_date = dt.date(\n",
    "            target_year, \n",
    "            target_month, \n",
    "            calendar.monthrange(target_year, target_month)[1]\n",
    "        ) + dt.timedelta(days=7)\n",
    "\n",
    "        applied_forecast = get_applied_forecast(\n",
    "            site_id, \n",
    "            first_date,\n",
    "            last_date,\n",
    "            connection=connection, \n",
    "            db_table=metadata_source.tables['forecasting_data']\n",
    "        )\n",
    "\n",
    "        # applied_forecast = get_applied_forecast(site_id, target_year, target_month, \n",
    "        #                                         connection=connection, db_table=metadata_source.tables['forecasts_applied'])\n",
    "        applied_forecast.columns = ['forecast']\n",
    "        # print('Forecast data of | {} records |'.format(len(applied_forecast)))\n",
    "        # print(applied_forecast.index.max())\n",
    "        # print(applied_forecast)\n",
    "\n",
    "        current_forecast_dates = [applied_forecast.index.max() + dt.timedelta(days=x) for x in range(1,4)]\n",
    "        current_forecast = get_current_forecast(site_id, current_forecast_dates, connection, metadata_source.tables['forecasts_applied']).to_frame()\n",
    "        current_forecast.columns = ['forecast']\n",
    "\n",
    "        # print(current_forecast.index.min())\n",
    "        # print(current_forecast.index.max())\n",
    "\n",
    "        forecast = pd.concat([applied_forecast, current_forecast])\n",
    "        forecast_data = pd.concat([forecast, mms_data.loc[mms_data.index >= forecast.index.min()]], axis=1, join='outer').reindex(columns=['yield', 'forecast'])\n",
    "\n",
    "        site_series = pd.Series(index=forecast_data.index, data=site)\n",
    "        latitude_series = pd.Series(index=forecast_data.index, data=latitude)\n",
    "        longitude_series = pd.Series(index=forecast_data.index, data=longitude)\n",
    "        region_series = pd.Series(index=forecast_data.index, data=region)\n",
    "        cluster_series = pd.Series(index=forecast_data.index, data=cluster)\n",
    "        capacity_dc_series = pd.Series(index=forecast_data.index, data=capacity_dc)\n",
    "        grid_capacity_series = pd.Series(index=forecast_data.index, data=grid_capacity)\n",
    "\n",
    "        site_data = pd.concat([\n",
    "                    site_series, \n",
    "                    latitude_series, \n",
    "                    longitude_series,\n",
    "                    region_series, \n",
    "                    cluster_series,\n",
    "                    capacity_dc_series,\n",
    "                    grid_capacity_series\n",
    "                ], \n",
    "                axis=1\n",
    "            )\n",
    "        site_data.columns = ['site', 'latitude', 'longitude', 'region', 'cluster', 'capacity_dc', 'grid_capacity']                       \n",
    "        site_data['date'] = site_data.index.strftime('%Y-%m-%d')\n",
    "        site_data['hour'] = site_data.index.hour + 1\n",
    "        site_data['datetime'] = site_data.index.strftime('%Y-%m-%dT%H:%M')\n",
    "        site_data['datetime_tz'] = site_data.index.tz_localize(pytz.utc).tz_convert(pytz.timezone('europe/kiev')).strftime('%Y-%m-%dT%H:%M%z')\n",
    "        \n",
    "        site_data = pd.concat([site_data, forecast_data], axis=1)\n",
    "\n",
    "        site_data['error'] = site_data['yield'] - site_data['forecast']\n",
    "        site_data['error_positive'] = site_data['error'].apply(lambda x: x * (x >= 0))\n",
    "        site_data['error_negative'] = site_data['error'].apply(lambda x: x * (x < 0))\n",
    "        site_data['error_abs'] = site_data['error'].apply(abs)\n",
    "        site_data['error_type'] = site_data['error'].apply(lambda x: 'negative' if x < 0 else 'positive')\n",
    "        \n",
    "        sites_data.append(site_data)\n",
    "        end = time.time()\n",
    "\n",
    "        print('{}: ок! Processing took {} seconds'.format(site, round(end - start, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(sites_data, ignore_index=True).fillna(0)\n",
    "data = data.drop_duplicates(keep='first')\n",
    "data.to_csv(target_folder + 'mart_operative_forecasting_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              site   latitude  longitude region  cluster  capacity_dc  \\\n",
      "8064  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8065  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8066  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8067  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8068  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "...            ...        ...        ...    ...      ...          ...   \n",
      "8476  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8492  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8493  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8494  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "8495  Myroliubivka  46.726252  32.286332     KH        5        11477   \n",
      "\n",
      "      grid_capacity        date  hour          datetime  \\\n",
      "8064           9000  2023-04-30    22  2023-04-30T21:30   \n",
      "8065           9000  2023-04-30    23  2023-04-30T22:30   \n",
      "8066           9000  2023-04-30    24  2023-04-30T23:30   \n",
      "8067           9000  2023-05-01     1  2023-05-01T00:30   \n",
      "8068           9000  2023-05-01     2  2023-05-01T01:30   \n",
      "...             ...         ...   ...               ...   \n",
      "8476           9000  2023-05-18     2  2023-05-18T01:30   \n",
      "8492           9000  2023-05-18    18  2023-05-18T17:30   \n",
      "8493           9000  2023-05-18    19  2023-05-18T18:30   \n",
      "8494           9000  2023-05-18    20  2023-05-18T19:30   \n",
      "8495           9000  2023-05-18    21  2023-05-18T20:30   \n",
      "\n",
      "                datetime_tz  yield  forecast  error  error_positive  \\\n",
      "8064  2023-05-01T00:30+0300    0.0       -23   23.0            23.0   \n",
      "8065  2023-05-01T01:30+0300    0.0       -23   23.0            23.0   \n",
      "8066  2023-05-01T02:30+0300    0.0       -23   23.0            23.0   \n",
      "8067  2023-05-01T03:30+0300    0.0       -23   23.0            23.0   \n",
      "8068  2023-05-01T04:30+0300   -1.0       -23   22.0            22.0   \n",
      "...                     ...    ...       ...    ...             ...   \n",
      "8476  2023-05-18T04:30+0300    0.0       -23    0.0             0.0   \n",
      "8492  2023-05-18T20:30+0300    0.0       -23    0.0             0.0   \n",
      "8493  2023-05-18T21:30+0300    0.0       -23    0.0             0.0   \n",
      "8494  2023-05-18T22:30+0300    0.0       -23    0.0             0.0   \n",
      "8495  2023-05-18T23:30+0300    0.0       -23    0.0             0.0   \n",
      "\n",
      "      error_negative  error_abs error_type  \n",
      "8064             0.0       23.0   positive  \n",
      "8065             0.0       23.0   positive  \n",
      "8066             0.0       23.0   positive  \n",
      "8067             0.0       23.0   positive  \n",
      "8068             0.0       22.0   positive  \n",
      "...              ...        ...        ...  \n",
      "8476             0.0        0.0   positive  \n",
      "8492             0.0        0.0   positive  \n",
      "8493             0.0        0.0   positive  \n",
      "8494             0.0        0.0   positive  \n",
      "8495             0.0        0.0   positive  \n",
      "\n",
      "[166 rows x 18 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "yield      -12.042169\n",
       "forecast   -23.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_26 = data.copy()\n",
    "# data_26 = data.loc[data.date == '2022-03-26']\n",
    "site = 'Myroliubivka'\n",
    "print(data_26.loc[(data_26.site == site) & (data_26['yield'] <= 0) & (data_26['forecast'] < 0)])\n",
    "data_26.loc[(data_26.site == site) & (data_26['yield'] <= 0) & (data_26['forecast'] < 0)][['yield', 'forecast']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting data to data mart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = data.copy()\n",
    "tuples_full = [tuple(x) for x in df_full.to_numpy()]\n",
    "# print(tuples_full[-2:])\n",
    "columns = list(df_full.columns)\n",
    "\n",
    "columns_unique = [columns[0], *columns[5:7]]\n",
    "# print(columns_unique)\n",
    "\n",
    "df_update = df_full.copy().drop(columns=columns_unique)\n",
    "tuples_update = [tuple(x) for x in df_update.to_numpy()]\n",
    "columns_update = list(df_update.columns)\n",
    "# print(columns_update)\n",
    "# SQL query to execute\n",
    "query_1 = 'INSERT INTO mart_operative_forecasting_result({}) VALUES {}'.format(','.join(columns), str(tuples_full).replace('[', '').replace(']', ''))\n",
    "query_2 = '''\n",
    "ON CONFLICT (site, date, hour) \n",
    "DO UPDATE SET\n",
    "latitude = excluded.latitude,\n",
    "longitude = excluded.longitude,\n",
    "region = excluded.region,\n",
    "cluster = excluded.cluster,\n",
    "datetime = excluded.datetime,\n",
    "datetime_tz = excluded.datetime_tz,\n",
    "yield = excluded.yield,\n",
    "forecast = excluded.forecast,\n",
    "error = excluded.error,\n",
    "error_positive = excluded.error_positive,\n",
    "error_negative = excluded.error_negative,\n",
    "error_abs = excluded.error_abs,\n",
    "error_type = excluded.error_type,\n",
    "capacity_dc = excluded.capacity_dc,\n",
    "grid_capacity = excluded.grid_capacity;'''\n",
    "# print(query_1, query_2)\n",
    "with engine_warehouse.connect() as connection:\n",
    "    connection.execute(query_1 + '\\n' + query_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('uce_daily')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f6094ec2d56ebdaae3d9c77f7ab32436394f8c6f1a90f5df9f6a393f513a0f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
